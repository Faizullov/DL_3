{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQwlunyE86kE"
      },
      "source": [
        "# 1. Практическое задание. Обучение полносвязной нейронной сети.\n",
        "\n",
        "**ФИО**: Файзуллов Айрат Рафагатович\n",
        "\n",
        "**Дедлайн**: 14 октября 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2284,
      "metadata": {
        "id": "UyCGfNgA86kF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "from glob import glob\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import gradcheck\n",
        "from torch.optim import Optimizer, Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFaWrLT4GkfY"
      },
      "source": [
        "## 1. Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R930jTYMG6nB"
      },
      "source": [
        "Если вам требуется работать с каким-нибубь набором данных (dataset), то прежде всего проверьте нет ли его среди встроенных наборов данных https://pytorch.org/vision/stable/datasets.html.\n",
        "\n",
        "В текущем домашнем задании мы будем работать с набором данных FashionMNIST. Он присутствует в списке встроенных наборов данных, однако мы воспользуемся реализацией только для удобного и быстрого способа скачать наборы данных. Ниже предлагается реализовать собственный класс для считывания, обработки и упаковки данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2285,
      "metadata": {
        "id": "LauGpMvGF5qr"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIT32OCMaLLP"
      },
      "source": [
        "Воспользуемся функцией загрузки данных из репозитория наборов данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2286,
      "metadata": {
        "id": "Fm_cf_hEIapm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b5607f-538a-4611-eca1-4ce783ef9020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t10k-images-idx3-ubyte\t   train-images-idx3-ubyte\n",
            "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\n",
            "t10k-labels-idx1-ubyte\t   train-labels-idx1-ubyte\n",
            "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "! ls data/FashionMNIST/raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2287,
      "metadata": {
        "id": "-xnY5xk1KBg2"
      },
      "outputs": [],
      "source": [
        "#https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
        "\n",
        "def load_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHIlWRV0aZnA"
      },
      "source": [
        "Для удобства PyTorch предоставляет ряд базовых классов `Dataset, DataLoader`, от которых предлагается отнаследоваться при разработке пользовательских классов. Базовый класс `Dataset` используется для загрузки и обработки данных, класс `DataLoader` используется для управления процессом загрузки данных, позволяет в многопоточном режиме загружать данные и упаковывать их.\n",
        "Эти вспомогательные классы находятся в модуле `torch.utils.data`.\n",
        "\n",
        "При наследовании от класса `torch.utils.data.Dataset` требуется переопределить метод `__len__`, который возвращает количество примеров в наборе данных, а также метод `__getitem__`, который позволяет получить доступ к примеру из набора данных по индексу."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jf2e5cPbJV2"
      },
      "source": [
        "Реализуем класс для FasionMnist.\n",
        "\n",
        "Элементами датасета должны являться пары '(np.array, int)', массив имеет размерность `(28, 28)`, тип элемента `np.float32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2288,
      "metadata": {
        "id": "snTBHRTQI1bc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class FashionMnist(Dataset):\n",
        "    def __init__(self, path, train=True, image_transform=None,\n",
        "                 label_transform=None):\n",
        "        if train:\n",
        "            images, labels = load_mnist(os.path.join(path,\"raw\"))\n",
        "        else:\n",
        "            images, labels = load_mnist(os.path.join(path,\"raw\"), kind=\"t10k\")\n",
        "\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        if not image_transform:\n",
        "          self._images = torch.tensor(images, dtype=torch.float32)\n",
        "        else:\n",
        "          self._images = image_transform(images.astype(np.float32))\n",
        "\n",
        "        if not label_transform:\n",
        "          self._labels = torch.tensor(labels)\n",
        "        else:\n",
        "          self._labels = label_transform(labels)\n",
        "\n",
        "    def __len__(self,):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        length = len(self._labels)\n",
        "\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        img = self._images[idx].reshape((28, 28))\n",
        "        label = self._labels[idx]\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2289,
      "metadata": {
        "id": "XVGv_2zBNfpz"
      },
      "outputs": [],
      "source": [
        "test_dataset = FashionMnist(\"data/FashionMNIST\", train=False)\n",
        "train_dataset = FashionMnist(\"data/FashionMNIST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JciETIfndiGR"
      },
      "source": [
        "Визуализируйте случайные элементы набора данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2290,
      "metadata": {
        "id": "wBky4UtmOS71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4e530c84-cc8b-4ce5-d4e8-e985f2cafb83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "322\n",
            "6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "\n",
        "decode_type = {\n",
        "    0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\",\n",
        "    6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
        "\n",
        "rand_i = np.random.randint(len(test_dataset))\n",
        "\n",
        "print(rand_i)\n",
        "print(test_dataset[rand_i][1].item())\n",
        "plt.imshow(test_dataset[rand_i][0], cmap='gray')\n",
        "plt.title(decode_type[test_dataset[rand_i][1].item()])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "320RvzL7PZrI"
      },
      "source": [
        "В конструктор `Dataset` можно передать объект `torchvision.transforms`, который позволяет преобразовать исходные данные. Преобразование `torchvision.transforms.ToTensor` позволяет преобразоать данные из типа `PIL Image` и `numpy.float32` в тип `torch.float32`\n",
        "\n",
        "Реализуйте собственную поддержку преобразований в `FashionMnist`. Проверьте, что приведение типов работает корректно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2291,
      "metadata": {
        "id": "pQsGbfXUTxcx"
      },
      "outputs": [],
      "source": [
        "class ToTensor:\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return torch.from_numpy(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2292,
      "metadata": {
        "id": "7BpFQ_Y2PoeI"
      },
      "outputs": [],
      "source": [
        "transform = ToTensor()\n",
        "\n",
        "test_dataset = FashionMnist(\"data/FashionMNIST\",\n",
        "                            train=False,\n",
        "                            image_transform=transform,\n",
        "                            label_transform=transform\n",
        "                            )\n",
        "train_dataset = FashionMnist(\"data/FashionMNIST\",\n",
        "                             image_transform=transform,\n",
        "                             label_transform=transform\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2293,
      "metadata": {
        "id": "ukLn9rY7SCsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d1dcf8-27dd-48e0-ae30-717fad5945f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of the data is <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "print(f\"The type of the data is {type(test_dataset[0][0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ltavx0QUMnm"
      },
      "source": [
        "Элементы набора данных могут быть объединены в пакеты (batch) явно и неявно. Если данные могут быть сконкатенированы или обЪединены каким-нибудь тривиальным способом, то можно не передавать никаких дополнительных парамертов в `torch.utils.data.Dataloader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2294,
      "metadata": {
        "id": "Cizxx6m0VAI7"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=15, num_workers=2, shuffle=True)\n",
        "batch = next(iter(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2295,
      "metadata": {
        "id": "SIZiPrBgVwGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9776d38c-e778-4280-e07b-26ff676a4b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the batch is 2\n",
            "The shape of the batch[0] is torch.Size([15, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(f\"The length of the batch is {len(batch)}\")\n",
        "print(f\"The shape of the batch[0] is {batch[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCggRyOQWVIx"
      },
      "source": [
        "Однако, если наша структура данных не позволяет нам использовать объединение по умолчанию, то можно написать собственную функцию, которая будет пакетировать данные.\n",
        "\n",
        "Реализуйте функцию, преобразующую последовательность элементов массива в пакет (batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2296,
      "metadata": {
        "id": "nQVh93fmWSjA"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    ###########################################################\n",
        "    ############# YOUR CODE HERE ##############################\n",
        "    ###########################################################\n",
        "    imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
        "    labels = torch.tensor([label[1].tolist() for label in batch])\n",
        "    return imgs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wij6F-vpe7KS"
      },
      "source": [
        "Убедитесть, что все работает корректно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2297,
      "metadata": {
        "id": "4dDtlvlCXNbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6e384d-a8ce-4292-9446-dfbf6fb3707a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        }
      ],
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=15, num_workers=2,\n",
        "                             shuffle=True, collate_fn=collate)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=15, num_workers=2,\n",
        "                              shuffle=True, collate_fn=collate)\n",
        "batch = next(iter(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2298,
      "metadata": {
        "id": "PFN00IyZXYaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353e2f13-aac5-4d5b-87b8-173e4ba074d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the batch is 2\n",
            "The shape of the batch[0] is torch.Size([15, 784])\n"
          ]
        }
      ],
      "source": [
        "print(f\"The length of the batch is {len(batch)}\")\n",
        "print(f\"The shape of the batch[0] is {batch[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHcumwuF86kL"
      },
      "source": [
        "## 2. Реализация модулей нейронной сети (15 баллов)\n",
        "\n",
        "В этом разделе мы полностью реализуем модули для полносвязанной сети.\n",
        "\n",
        "Для начала нам понадобится реализовать прямой и обратный проход через слои.\n",
        "\n",
        "Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzhCW5HcfmKd"
      },
      "source": [
        "Сначала, мы реализуем функцию и её градиент."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2299,
      "metadata": {
        "id": "rcqeFXxsFGQO"
      },
      "outputs": [],
      "source": [
        "class IdentityFunction(Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        return grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc6Rtczffxv4"
      },
      "source": [
        "Разработанную функцию обернем классом `IdentityLayer`, все слои в `PyTorch` должны быть наследниками базового класса `nn.Module()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2300,
      "metadata": {
        "id": "SiqeRVcM86kM"
      },
      "outputs": [],
      "source": [
        "class IdentityLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        # An identity layer does nothing\n",
        "        super().__init__()\n",
        "        self.identity = IdentityFunction.apply\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # An identity layer just returns whatever it gets as input.\n",
        "        return self.identity(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4IoM_pX86kQ"
      },
      "source": [
        "\n",
        "### 2.1 Функция активации ReLU\n",
        "Для начала реализуем функцию активации, слой нелинейности `ReLU(x) = max(x, 0)`. Параметров у слоя нет. Метод `forward` должен вернуть результат поэлементного применения `ReLU` к входному массиву, метод `backward` - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить в `ctx`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2301,
      "metadata": {
        "id": "U9qrJ47xY6H0"
      },
      "outputs": [],
      "source": [
        "class ReLUFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return torch.max(torch.zeros(input.shape), input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x = ctx.saved_tensors\n",
        "        return grad_output * torch.where(x[0] > 0, 1.0, 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2302,
      "metadata": {
        "id": "y09ZVCsT86kT"
      },
      "outputs": [],
      "source": [
        "class ReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        super().__init__()\n",
        "        self.relu = ReLUFunction.apply\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        return self.relu(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RwKEUp_V3-L"
      },
      "source": [
        "Не забываем после реализации функции проверить градиент, испльзуя функцию `gradcheck`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2303,
      "metadata": {
        "id": "K0RnDQZCXXZn"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "relu = ReLU()\n",
        "x = torch.randn(2, 6, requires_grad=True)\n",
        "assert gradcheck(relu, x, eps=1e-4, atol=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2304,
      "metadata": {
        "id": "vUmbmR4iXurd"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "torch_relu = torch.nn.ReLU()\n",
        "our_relu = ReLU()\n",
        "\n",
        "assert torch.norm(torch_relu(x.float()) - our_relu(x)) < 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojTR4GFd86kY"
      },
      "source": [
        "### 2.2 Линейный слой (linear, fully-connected)\n",
        "Далее реализуем полносвязный слой без нелинейности. У слоя два набора параметра: матрица весов (weights) и вектор смещения (bias)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2305,
      "metadata": {
        "id": "fBl34oykbcBf"
      },
      "outputs": [],
      "source": [
        "class LinearFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inp, weight, bias):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        ctx.save_for_backward(inp, weight, bias)\n",
        "        output = inp  @ weight + bias\n",
        "        return output\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        (inp, weight, bias) = ctx.saved_tensors\n",
        "\n",
        "        grad_input = grad_output @ weight.t()\n",
        "        grad_weight = inp.t() @ grad_output\n",
        "\n",
        "        grad_bias = torch.ones(len(bias))\n",
        "        return grad_input, grad_weight, grad_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2306,
      "metadata": {
        "id": "YbN5JOc886kZ"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_units, output_units):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "        self.weight = nn.Parameter(torch.normal(0, 1/256, size=(input_units, output_units)))\n",
        "        self.bias = nn.Parameter(torch.zeros(output_units))\n",
        "        self.linear = LinearFunction.apply\n",
        "\n",
        "\n",
        "    def forward(self,inp):\n",
        "\n",
        "        return self.linear(inp, self.weight, self.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gToXI43WYMDv"
      },
      "source": [
        "Проверим градиент, а также сравним с работой нашего модуля с имплементированным в `PyTorch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et7mKX5xZfMI"
      },
      "source": [
        "Проверка градиента:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2307,
      "metadata": {
        "id": "PxLVozqLZb_H"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "\n",
        "linear = Linear(6, 2)\n",
        "x = torch.randn((2, 6), requires_grad=True)\n",
        "\n",
        "assert gradcheck(linear, x, eps=1e-3, atol=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW8ppOciZndN"
      },
      "source": [
        "Сравнение с `PyTorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2308,
      "metadata": {
        "id": "IIR9svs7Zmq0"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "x = torch.randn((2, 6), requires_grad=True)\n",
        "torch_linear = torch.nn.Linear(6, 2)\n",
        "our_linear = Linear(6, 2)\n",
        "\n",
        "weight_q = torch.normal(0, 1, size=(6, 2), requires_grad=True)\n",
        "bias = torch.zeros(2, requires_grad=True)\n",
        "\n",
        "state_dict = OrderedDict([(\"weight\", weight_q.T), (\"bias\", bias)])\n",
        "\n",
        "torch_linear.load_state_dict(state_dict)\n",
        "state_dict = OrderedDict([(\"weight\", weight_q), (\"bias\", bias)])\n",
        "our_linear.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "assert torch.norm(torch_linear(x.float()) - our_linear(x)) < 1e-5\n",
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dyhDg0D86kt"
      },
      "source": [
        "### 2.3 LogSoftmax (Log + Softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL0HLGH86ku"
      },
      "source": [
        "Для решения задачи многоклассовой классификации обычно используют `softmax` в качестве нелинейности на последнем слое, чтобы получить \"оценку\" вероятности классов для каждого объекта:$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$В этом случае удобно оптимизировать логарифм правдоподобия:$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$В таком виде ее удобно реализовывать.\n",
        "\n",
        "Реализуйте слой `LogSoftmax` (без параметров). Метод `forward` должен вычислять логарифм от `softmax`, а метод `backward` - пропускать градиенты. В общем случае в промежуточных вычислениях `backward` получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде. Поэтому мы будем предполагать, что аргумент `grad_output` - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица).\n",
        "\n",
        "Комментарий: разобраться `Log-Sum-Exp trick`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2309,
      "metadata": {
        "id": "xSV3XD0N86ky"
      },
      "outputs": [],
      "source": [
        "class LogSoftmaxFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inp):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "\n",
        "        # y = torch.exp(inp)\n",
        "        # s = torch.sum(y)\n",
        "        # y = torch.log(s\n",
        "\n",
        "        y = inp - torch.log(torch.sum(torch.exp(inp), axis=1)).view(-1, 1)\n",
        "\n",
        "        ctx.save_for_backward(y)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        y = ctx.saved_tensors\n",
        "        grad_input = grad_output - grad_output.sum(axis=1, keepdims=True) * torch.exp(y[0])\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2310,
      "metadata": {
        "id": "z7OnC6o_z2vg"
      },
      "outputs": [],
      "source": [
        "class LogSoftmax(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        self.logsoftmax = LogSoftmaxFunction.apply\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        return  self.logsoftmax(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke4G67cddjVA"
      },
      "source": [
        "Проверка градиентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2311,
      "metadata": {
        "id": "cfgkVgGbrQEv"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "\n",
        "\n",
        "logsoftmax = LogSoftmax()\n",
        "x = torch.randn((15, 6), requires_grad=True)\n",
        "\n",
        "assert gradcheck(logsoftmax, x, eps=1e-2, atol=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "our_softmax = LogSoftmax()\n",
        "print(torch_softmax(x.float()))\n",
        "print(our_softmax(x.float()))\n",
        "print(torch.norm(torch_softmax(x.float()) - our_softmax(x.float())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-dEcD3mByoB",
        "outputId": "76e6df05-fd4d-4e7d-ef47-645d6edc40f9"
      },
      "execution_count": 2312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.8947, -1.4744, -2.3562, -1.7705, -1.1859, -2.9912],\n",
            "        [-1.2372, -2.5032, -2.2864, -1.8627, -1.1331, -3.0143],\n",
            "        [-2.6036, -1.6817, -1.8773, -1.0379, -1.7427, -2.8523],\n",
            "        [-0.5816, -1.6990, -2.2387, -3.1959, -2.3699, -4.0677],\n",
            "        [-3.1274, -3.4741, -4.0683, -0.1943, -3.5101, -2.9048],\n",
            "        [-2.9783, -0.3165, -2.4894, -4.8924, -2.7001, -2.7684],\n",
            "        [-2.2611, -0.6211, -2.9859, -3.2134, -2.4607, -1.7019],\n",
            "        [-3.2993, -2.9856, -1.2170, -3.2425, -1.0551, -1.4729],\n",
            "        [-1.8580, -2.3331, -1.6354, -2.4970, -0.8950, -2.7937],\n",
            "        [-3.4977, -0.4940, -2.0371, -2.8931, -2.8546, -2.1530],\n",
            "        [-3.2599, -2.5496, -1.9801, -2.0499, -1.3608, -1.0210],\n",
            "        [-1.7687, -1.4234, -4.0599, -1.0690, -4.8270, -1.5144],\n",
            "        [-3.5284, -2.1272, -1.4213, -1.7543, -1.7222, -1.3533],\n",
            "        [-3.0849, -1.0393, -1.4763, -1.6698, -3.5066, -1.8722],\n",
            "        [-2.3879, -1.3004, -1.6742, -2.4993, -2.1524, -1.3866]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[-1.8947, -1.4744, -2.3562, -1.7705, -1.1859, -2.9912],\n",
            "        [-1.2372, -2.5032, -2.2864, -1.8627, -1.1331, -3.0143],\n",
            "        [-2.6036, -1.6817, -1.8773, -1.0379, -1.7427, -2.8523],\n",
            "        [-0.5816, -1.6990, -2.2387, -3.1959, -2.3699, -4.0677],\n",
            "        [-3.1274, -3.4741, -4.0683, -0.1943, -3.5101, -2.9048],\n",
            "        [-2.9783, -0.3165, -2.4894, -4.8924, -2.7001, -2.7684],\n",
            "        [-2.2611, -0.6211, -2.9859, -3.2134, -2.4607, -1.7019],\n",
            "        [-3.2993, -2.9856, -1.2170, -3.2425, -1.0551, -1.4729],\n",
            "        [-1.8580, -2.3331, -1.6354, -2.4970, -0.8950, -2.7937],\n",
            "        [-3.4977, -0.4940, -2.0371, -2.8931, -2.8546, -2.1530],\n",
            "        [-3.2599, -2.5496, -1.9801, -2.0499, -1.3608, -1.0210],\n",
            "        [-1.7687, -1.4234, -4.0599, -1.0690, -4.8270, -1.5144],\n",
            "        [-3.5284, -2.1272, -1.4213, -1.7543, -1.7222, -1.3533],\n",
            "        [-3.0849, -1.0393, -1.4763, -1.6698, -3.5066, -1.8722],\n",
            "        [-2.3879, -1.3004, -1.6742, -2.4993, -2.1524, -1.3866]],\n",
            "       grad_fn=<LogSoftmaxFunctionBackward>)\n",
            "tensor(1.2801e-06, grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sn2M_Q086k2"
      },
      "source": [
        "### 2.4 Dropout\n",
        "Реализуйте слой Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2313,
      "metadata": {
        "id": "qCLECy1y86k3"
      },
      "outputs": [],
      "source": [
        "class DropoutFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inp, p):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        x = torch.bernoulli((1-p) * torch.ones_like(inp)) / (1 - p)\n",
        "        ctx.save_for_backward(x, torch.tensor(p))\n",
        "\n",
        "        return inp * x\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        (x, p) = ctx.saved_tensors\n",
        "        return grad_output * x, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2314,
      "metadata": {
        "id": "pS5z_mth1ZAZ"
      },
      "outputs": [],
      "source": [
        "class Dropout(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        self.dropout = DropoutFunction.apply\n",
        "        self.p = p\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        if not self.training:\n",
        "          output = self.dropout(input, self.p)\n",
        "        else:\n",
        "          output = input\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLLxHX-HRgFb"
      },
      "execution_count": 2314,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyPPzkCI86k7"
      },
      "source": [
        "### 2.5 CrossEntropy\n",
        "\n",
        "При решении задачи многоклассовой классификации мы будет использовать в качестве функции потерь кроссэнтропию. Реализуйте функцию потерь. В разделе 2.3 приведены полезные формулы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2315,
      "metadata": {
        "id": "KotXWXnT3-j5"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, activations, target):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        ctx.save_for_backward(activations, target)\n",
        "        loss = torch.mean(-activations[range(activations.size(0)), target])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        (activations, target) = ctx.saved_tensors\n",
        "\n",
        "        grad_input = torch.zeros_like(activations)\n",
        "        grad_input[range(activations.shape[0]), target] = True\n",
        "\n",
        "        grad_input = -grad_input * grad_output / len(target)\n",
        "\n",
        "\n",
        "        return grad_input, None\n",
        "\n",
        "\n",
        "class CrossEntropy(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        self.crossentropy = CrossEntropyFunction.apply\n",
        "\n",
        "    def forward(self, activations, target):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        return self.crossentropy(activations, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFNcOfVNesCC"
      },
      "source": [
        "Проверка градиентов."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "our_crossentr = CrossEntropy()\n",
        "x = torch.randn((2, 6), requires_grad=True)\n",
        "trgt = torch.randint(6, (2, ))\n",
        "\n",
        "assert gradcheck(our_crossentr, (x, trgt), eps=1e-2, atol=1e-3)"
      ],
      "metadata": {
        "id": "wmn3q9bslIqa"
      },
      "execution_count": 2316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2317,
      "metadata": {
        "id": "lXQr7sllKi8k"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "############# YOUR CODE HERE ##############################\n",
        "###########################################################\n",
        "\n",
        "torch_crossentr = torch.nn.CrossEntropyLoss()\n",
        "our_crossentr = CrossEntropy()\n",
        "x = torch.randn((2, 6), requires_grad=True)\n",
        "trgt = torch.randint(6, (2, ))\n",
        "\n",
        "assert torch.norm(torch_crossentr(x.float(), trgt).sum() - our_crossentr(our_softmax(x.float()), trgt)) < 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzKDpyE86lg"
      },
      "source": [
        "## 3. Сборка и обучение нейронной сети (5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPqlZfj_86lg"
      },
      "source": [
        "Реализуйте произвольную нейросеть, состоящую из ваших блоков. Она должна состоять из нескольких полносвязанных слоев."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2318,
      "metadata": {
        "id": "tkESXVD87sM8"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_layers_size=32, num_layers=5,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(Linear(input_size, hidden_layers_size))\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        self.layers.append(ReLU())\n",
        "\n",
        "        for _ in range(num_layers-2):\n",
        "          self.layers.append(Linear(hidden_layers_size, hidden_layers_size))\n",
        "          self.layers.append(Dropout(0.5))\n",
        "          self.layers.append(ReLU())\n",
        "\n",
        "        self.layers.append(Linear(hidden_layers_size, num_classes))\n",
        "        self.layers.append(LogSoftmax())\n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        for layer in self.layers:\n",
        "            inp = layer(inp)\n",
        "        return inp\n",
        "\n",
        "    def predict(self, inp):\n",
        "        ###########################################################\n",
        "        ############# YOUR CODE HERE ##############################\n",
        "        ###########################################################\n",
        "        for layer in self.layers:\n",
        "          inp = layer(inp)\n",
        "        return torch.argmax(inp, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj-eEvMFfbSo"
      },
      "source": [
        "Ниже приведены функции, реализующие обучение нейронной сети. В данном задании их предлагается просто переиспользовать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2319,
      "metadata": {
        "id": "vDhcoCB4OpXE"
      },
      "outputs": [],
      "source": [
        "class EmptyContext:\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2320,
      "metadata": {
        "id": "8AsWblqIOquI"
      },
      "outputs": [],
      "source": [
        "# accuract metric for our classififcation\n",
        "def accuracy(model_labels, labels):\n",
        "  #print(model_labels)\n",
        "  return torch.mean((model_labels == labels).float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2321,
      "metadata": {
        "id": "Oy33FHuv_Us-"
      },
      "outputs": [],
      "source": [
        "def perform_epoch(model, loader, criterion,\n",
        "                optimizer=None, device=None):\n",
        "    is_train = optimizer is not None\n",
        "    model = model.to(device)\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_n = 0\n",
        "    with EmptyContext() if is_train else torch.no_grad():\n",
        "        for batch_data, batch_labels in loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            model_labels = model(batch_data)\n",
        "            model_prediction = model.predict(batch_data)\n",
        "            new_loss = criterion(model_labels, batch_labels)\n",
        "            if is_train:\n",
        "              optimizer.zero_grad()\n",
        "              new_loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "            one_batch_loss = float(criterion(model_labels, batch_labels))\n",
        "            one_batch_acc = accuracy(model_prediction, batch_labels)\n",
        "\n",
        "            total_loss += one_batch_loss\n",
        "            total_acc += one_batch_acc\n",
        "            total_n += 1\n",
        "    return (total_loss / total_n, total_acc / total_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtamPEJZgOY5"
      },
      "source": [
        "Теперь обучим нашу нейронную сеть. В данном разделе будем использовать оптимизатор `Adam` с параметрами по умолчанию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2322,
      "metadata": {
        "id": "mEcyUJJI_aAn"
      },
      "outputs": [],
      "source": [
        "model     = Network()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = CrossEntropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2323,
      "metadata": {
        "id": "OBADDSi0_jx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114445c7-44ac-4808-f05a-c956758a782c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 0 : loss 1.1476441325452178, accuracy 0.5338825583457947\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 : loss 0.5491086401510984, accuracy 0.8086344599723816\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 2 : loss 0.49225384333124383, accuracy 0.8311514258384705\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 3 : loss 0.469542593490798, accuracy 0.8375845551490784\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 4 : loss 0.4526301876233774, accuracy 0.8421180248260498\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 5 : loss 0.44637916130851957, accuracy 0.8446509838104248\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 6 : loss 0.4319304634318687, accuracy 0.8498013019561768\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 7 : loss 0.43346671729371883, accuracy 0.8492845892906189\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 8 : loss 0.428495947082527, accuracy 0.8509683609008789\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 9 : loss 0.4253551564901136, accuracy 0.8520345091819763\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss, acc = perform_epoch(model, train_dataloader, criterion,\n",
        "                                optimizer=optimizer, device=device)\n",
        "    print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2326,
      "metadata": {
        "id": "JxrvNtm3YSJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7391cf-b14d-4b4e-84f5-6ff6fdad47c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 9 : loss 0.4253551564901136, accuracy 0.8520345091819763\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что модель обучилась"
      ],
      "metadata": {
        "id": "bStJKrZvgHv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Network(num_layers=4)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = CrossEntropy()"
      ],
      "metadata": {
        "id": "9DCRYOKsgMke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS41CQj0Y6Q8"
      },
      "source": [
        "Дальше:\n",
        "- Проведите эксперименты с числом слоев.\n",
        "- Постройте графики зависимости качества модели на тренировочной и тестовой выборках от числа слоев. Для получения статистически значимых результатов повторите эксперименты несколько раз.\n",
        "- Сделайте выводы."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проведём эксперименты с числом слоёв"
      ],
      "metadata": {
        "id": "SfXHK6kKi44Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Network(num_layers=4)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = CrossEntropy()\n",
        "\n",
        "save_acc = []\n",
        "save_loss = []"
      ],
      "metadata": {
        "id": "-0rIIKcKi_4r"
      },
      "execution_count": 2342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss, acc = perform_epoch(model, train_dataloader, criterion,\n",
        "                                optimizer=optimizer, device=device)\n",
        "    print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "    save_acc.append(acc)\n",
        "    save_loss.append(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMVYhENrjZvC",
        "outputId": "5b130fb1-96ea-46cc-bf08-8426da47ea41"
      },
      "execution_count": 2343,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 0 : loss 0.7395468326844274, accuracy 0.7164774537086487\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 : loss 0.48700648699887095, accuracy 0.8310511112213135\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 2 : loss 0.4477982891011052, accuracy 0.8448506593704224\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 3 : loss 0.4311057607653784, accuracy 0.8531351685523987\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 4 : loss 0.4194346586811589, accuracy 0.8554011583328247\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 5 : loss 0.4150186154625844, accuracy 0.8569678068161011\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 6 : loss 0.4007925677623134, accuracy 0.8621342778205872\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 7 : loss 0.4039252343769185, accuracy 0.8638843297958374\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 8 : loss 0.39427274736599066, accuracy 0.8665170073509216\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 9 : loss 0.38809271637513304, accuracy 0.8694499731063843\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Network(num_layers=3)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = CrossEntropy()\n",
        "\n",
        "save_acc3 = []\n",
        "save_loss3 = []"
      ],
      "metadata": {
        "id": "JGMfB00em_Vj"
      },
      "execution_count": 2344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss, acc = perform_epoch(model, train_dataloader, criterion,\n",
        "                                optimizer=optimizer, device=device)\n",
        "    print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "    save_acc3.append(acc)\n",
        "    save_loss3.append(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwX4ukYPm_98",
        "outputId": "9811b604-1772-4a23-bb89-94a9ec9c7023"
      },
      "execution_count": 2345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 0 : loss 0.5572166333051864, accuracy 0.797798752784729\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 : loss 0.4480183974970132, accuracy 0.840435266494751\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 2 : loss 0.4258654814339243, accuracy 0.8489507436752319\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 3 : loss 0.40912252018880096, accuracy 0.8529180884361267\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 4 : loss 0.4000873397201067, accuracy 0.8588838577270508\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 5 : loss 0.3969304754562036, accuracy 0.8601667284965515\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 6 : loss 0.3946626074768137, accuracy 0.8616506457328796\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 7 : loss 0.39118483250960706, accuracy 0.8629835844039917\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 8 : loss 0.3839629437113763, accuracy 0.8643172979354858\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 9 : loss nan, accuracy 0.10225153714418411\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Network(num_layers=2)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = CrossEntropy()\n",
        "\n",
        "save_acc2 = []\n",
        "save_loss2 = []"
      ],
      "metadata": {
        "id": "-HVcsUDTpAA5"
      },
      "execution_count": 2346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss, acc = perform_epoch(model, train_dataloader, criterion,\n",
        "                                optimizer=optimizer, device=device)\n",
        "    print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "    save_acc2.append(acc)\n",
        "    save_loss2.append(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dVh0dC3pJX6",
        "outputId": "245c3f5e-28cd-4a26-dca4-7fac143db11d"
      },
      "execution_count": 2347,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 0 : loss 0.7032291752719321, accuracy 0.7172958850860596\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 : loss 0.6607059539975598, accuracy 0.7316970229148865\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 2 : loss inf, accuracy 0.3187549114227295\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 3 : loss inf, accuracy 0.10000135004520416\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 4 : loss inf, accuracy 0.10000143200159073\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 5 : loss inf, accuracy 0.10000135749578476\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 6 : loss inf, accuracy 0.10000155121088028\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 7 : loss inf, accuracy 0.10000139474868774\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 8 : loss inf, accuracy 0.10000152885913849\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 9 : loss inf, accuracy 0.1000015065073967\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Network(num_layers=1)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = CrossEntropy()\n",
        "\n",
        "save_acc6 = []\n",
        "save_loss6 = []"
      ],
      "metadata": {
        "id": "oQlgMWsCt7nm"
      },
      "execution_count": 2357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss, acc = perform_epoch(model, train_dataloader, criterion,\n",
        "                                optimizer=optimizer, device=device)\n",
        "    print(f\"Epoch - {epoch} : loss {loss}, accuracy {acc}\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "    save_acc6.append(acc)\n",
        "    save_loss6.append(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlE76ZFlt_ZY",
        "outputId": "f006cc4d-35fd-4aee-efde-fa8e6f71ff23"
      },
      "execution_count": 2358,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 0 : loss 0.6492692687623203, accuracy 0.77011638879776\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 : loss 0.5930316600701772, accuracy 0.7954850196838379\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 2 : loss 0.5864302387251519, accuracy 0.7993838787078857\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 3 : loss 0.5670293176553678, accuracy 0.8043672442436218\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 4 : loss 0.5762810328477063, accuracy 0.8036169409751892\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 5 : loss 0.5614959933268837, accuracy 0.8075357675552368\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 6 : loss 0.5711254649232141, accuracy 0.803518533706665\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 7 : loss 0.5622398548149504, accuracy 0.8097509145736694\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 8 : loss 0.584019125275896, accuracy 0.7995696663856506\n",
            "Current learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n",
            "<ipython-input-2296-a60754b173b7>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  imgs = torch.tensor([np.ravel(img[0].tolist()) for img in batch]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 9 : loss 0.5561237859255634, accuracy 0.8124681115150452\n",
            "Current learning rate: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.arange(10)\n",
        "x1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeLVr3qCmZBP",
        "outputId": "1903fd82-2cc8-43bf-bf5f-093cd5986598"
      },
      "execution_count": 2348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 2348
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeFWYDDFo5p1",
        "outputId": "5ad73438-78ae-43c8-8a9e-ade19232a84c"
      },
      "execution_count": 2349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.7165),\n",
              " tensor(0.8311),\n",
              " tensor(0.8449),\n",
              " tensor(0.8531),\n",
              " tensor(0.8554),\n",
              " tensor(0.8570),\n",
              " tensor(0.8621),\n",
              " tensor(0.8639),\n",
              " tensor(0.8665),\n",
              " tensor(0.8694)]"
            ]
          },
          "metadata": {},
          "execution_count": 2349
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x1, save_acc, label=\"4 labels\")\n",
        "plt.plot(x1, save_acc3, label=\"3 labels\")\n",
        "plt.plot(x1, save_acc2, label=\"2 labels\")\n",
        "plt.plot(x1, save_acc6, label=\"1 label\")\n",
        "# plt.plot(x1, b_basic, label = \"basic\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "FaCjGkERmmLV",
        "outputId": "9f02bc12-0453-438c-e5fb-a1b7de1ea00f"
      },
      "execution_count": 2359,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод"
      ],
      "metadata": {
        "id": "OzRuJHFKwsP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно сделать вывод, что на 4 слоях сеть показывает наилучшее предсказание.\n",
        "\n",
        "Также можно заметить, что с 2 и 3 слоями она дала нам плохой результат. Скорее всего это связано с начальной инициализацией весов. Стоило их инициализорвать методом, предназначенным для функции активации ReLU.\n",
        "\n",
        "Ещё есть интересное наблюдение: даже с 1 слоем сеть довольно хорошо обучилась и теперь способно с неплохой точностью распознавать картинки."
      ],
      "metadata": {
        "id": "h9n4DM7fw3ON"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3vcONKQ86mu"
      },
      "source": [
        "## 4. Бонусная часть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuhxy7aW86mu"
      },
      "source": [
        "### 4.1 Реализация метода оптимизации (3 + 3 балла).\n",
        "Реализуйте сами метод оптимизации  для рассмотренной выше архитектуры. Вы можете выбрать произвольный метод от градиентного спуска до современных вариантов. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с Adam.\n",
        "\n",
        "**Дополнительные баллы** вы получите, если метод будет уникален среди сдавших задание."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2325,
      "metadata": {
        "id": "GYMu0uF1p6o0"
      },
      "outputs": [],
      "source": [
        "class SotaOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        defaults = dict(lr=lr)\n",
        "        super(SotaOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SotaOptimizer, self).__setstate__(state)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self,):\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.data.add_(-lr*p.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aabYCykXL7"
      },
      "source": [
        "### 4.2 Реализация современной функции активации (2 + 2 балла).\n",
        "Реализуйте одну из активаций, предложенных на лекции или в статье. Например, `Hardswish`. Сравните сеть с вашей активацией и с `ReLU`.\n",
        "\n",
        "**Дополнительные баллы** вы получите, если функция будет уникальна среди сдавших задание."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}